{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZWy7iSQtw79"
      },
      "source": [
        "#Data generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "U_hBr8mEtuV2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def ds_generator_historical(data_ts, batch_s, fh):\n",
        "    # TS samples\n",
        "    ts_x = data_ts[:, :-fh]\n",
        "    ts_y = data_ts[:, -fh:]\n",
        "    train_length = len(ts_x)\n",
        "\n",
        "    # Training data\n",
        "    data_train = tf.data.Dataset.from_tensor_slices((ts_x, ts_y))\n",
        "    data_train = data_train.shuffle(buffer_size=train_length)\n",
        "    data_train = data_train.repeat()\n",
        "    data_train = data_train.batch(batch_size=batch_s, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data_train = data_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return data_train\n",
        "\n",
        "def ds_generator_historical_calendar(data_ts, data_calendar, batch_s, fh):\n",
        "    # TS samples\n",
        "    ts_x = data_ts[:, :-fh]\n",
        "    ts_y = data_ts[:, -fh:]\n",
        "    train_length = len(ts_x)\n",
        "\n",
        "    # Training data\n",
        "    ds_x_ts = tf.data.Dataset.from_tensor_slices((ts_x))\n",
        "    ds_x_exog = tf.data.Dataset.from_tensor_slices((data_calendar))\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices((ts_y))\n",
        "\n",
        "    data_train = tf.data.Dataset.zip(((ds_x_ts, ds_x_exog), ds_y))\n",
        "    data_train = data_train.shuffle(buffer_size=train_length)\n",
        "    data_train = data_train.repeat()\n",
        "    data_train = data_train.batch(batch_size=batch_s, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data_train = data_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return data_train\n",
        "\n",
        "def ds_generator_historical_weather(data_ts, data_weather, batch_s, fh):\n",
        "    # TS samples\n",
        "    ts_x = data_ts[:, :-fh]\n",
        "    ts_y = data_ts[:, -fh:]\n",
        "    train_length = len(ts_x)\n",
        "\n",
        "    # Training data\n",
        "    ds_x_ts = tf.data.Dataset.from_tensor_slices((ts_x))\n",
        "    ds_x_weather = tf.data.Dataset.from_tensor_slices((data_weather))\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices((ts_y))\n",
        "\n",
        "    # data_train = tf.data.Dataset.zip(((ds_x_ts, ds_x_weather), ds_y))\n",
        "    data_train = tf.data.Dataset.zip(((ds_x_ts, ds_x_weather), ds_y))\n",
        "    data_train = data_train.shuffle(buffer_size=train_length)\n",
        "    data_train = data_train.repeat()\n",
        "    data_train = data_train.batch(batch_size=batch_s, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data_train = data_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return data_train\n",
        "\n",
        "def ds_generator_historical_country(data_ts, data_country, batch_s, fh):\n",
        "    # TS samples\n",
        "    ts_x = data_ts[:, :-fh]\n",
        "    ts_y = data_ts[:, -fh:]\n",
        "    train_length = len(ts_x)\n",
        "\n",
        "    # Training data\n",
        "    ds_x_ts = tf.data.Dataset.from_tensor_slices((ts_x))\n",
        "    ds_x_country = tf.data.Dataset.from_tensor_slices((data_country))\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices((ts_y))\n",
        "\n",
        "    # data_train = tf.data.Dataset.zip(((ds_x_ts, ds_x_weather), ds_y))\n",
        "    data_train = tf.data.Dataset.zip(((ds_x_ts, ds_x_country), ds_y))\n",
        "    data_train = data_train.shuffle(buffer_size=train_length)\n",
        "    data_train = data_train.repeat()\n",
        "    data_train = data_train.batch(batch_size=batch_s, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data_train = data_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return data_train\n",
        "\n",
        "def ds_generator_historical_calendar_country(data_ts, data_calendar, data_country, batch_s, fh):\n",
        "    # TS samples\n",
        "    ts_x = data_ts[:, :-fh]\n",
        "    ts_y = data_ts[:, -fh:]\n",
        "    train_length = len(ts_x)\n",
        "\n",
        "    # Training data\n",
        "    ds_x_ts = tf.data.Dataset.from_tensor_slices((ts_x))\n",
        "    ds_x_calendar = tf.data.Dataset.from_tensor_slices((data_calendar))\n",
        "    ds_x_country = tf.data.Dataset.from_tensor_slices((data_country))\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices((ts_y))\n",
        "\n",
        "    data_train = tf.data.Dataset.zip(((ds_x_ts, ds_x_calendar, ds_x_country), ds_y))\n",
        "    data_train = data_train.shuffle(buffer_size=train_length)\n",
        "    data_train = data_train.repeat()\n",
        "    data_train = data_train.batch(batch_size=batch_s, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data_train = data_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return data_train\n",
        "\n",
        "def ds_generator_historical_calendar_weather(data_ts, data_calendar, data_weather, batch_s, fh):\n",
        "    # TS samples\n",
        "    ts_x = data_ts[:, :-fh]\n",
        "    ts_y = data_ts[:, -fh:]\n",
        "    train_length = len(ts_x)\n",
        "\n",
        "    # Training data\n",
        "    ds_x_ts = tf.data.Dataset.from_tensor_slices((ts_x))\n",
        "    ds_x_calendar = tf.data.Dataset.from_tensor_slices((data_calendar))\n",
        "    ds_x_weather = tf.data.Dataset.from_tensor_slices((data_weather))\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices((ts_y))\n",
        "\n",
        "    data_train = tf.data.Dataset.zip(((ds_x_ts, ds_x_calendar, ds_x_weather), ds_y))\n",
        "    data_train = data_train.shuffle(buffer_size=train_length)\n",
        "    data_train = data_train.repeat()\n",
        "    data_train = data_train.batch(batch_size=batch_s, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data_train = data_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return data_train\n",
        "\n",
        "def ds_generator_historical_calendar_country_weather(data_ts, data_calendar, data_country, data_weather, batch_s, fh):\n",
        "    # TS samples\n",
        "    ts_x = data_ts[:, :-fh]\n",
        "    ts_y = data_ts[:, -fh:]\n",
        "    train_length = len(ts_x)\n",
        "\n",
        "    # Training data\n",
        "    ds_x_ts = tf.data.Dataset.from_tensor_slices((ts_x))\n",
        "    ds_x_calendar = tf.data.Dataset.from_tensor_slices((data_calendar))\n",
        "    ds_x_country = tf.data.Dataset.from_tensor_slices((data_country))\n",
        "    ds_x_weather = tf.data.Dataset.from_tensor_slices((data_weather))\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices((ts_y))\n",
        "\n",
        "    data_train = tf.data.Dataset.zip(((ds_x_ts, ds_x_calendar, ds_x_country, ds_x_weather), ds_y))\n",
        "    data_train = data_train.shuffle(buffer_size=train_length)\n",
        "    data_train = data_train.repeat()\n",
        "    data_train = data_train.batch(batch_size=batch_s, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data_train = data_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return data_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-uEtPr3rpwa"
      },
      "source": [
        "#Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lfz8p2zvraSa"
      },
      "outputs": [],
      "source": [
        "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "from keras.backend import softmax\n",
        "\n",
        "\n",
        "class DotProductAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, queries, keys, values, d_k, mask=None):\n",
        "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "\n",
        "        # Apply mask to the attention scores\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        "\n",
        "        # Computing the weights by a softmax operation\n",
        "        weights = softmax(scores)\n",
        "\n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return matmul(weights, values)\n",
        "\n",
        "# Implementing the Multi-Head Attention\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
        "        self.heads = h  # Number of attention heads to use\n",
        "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
        "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
        "        self.d_model = d_model  # Dimensionality of the model\n",
        "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
        "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
        "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
        "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
        "\n",
        "    def reshape_tensor(self, x, heads, flag):\n",
        "        if flag:\n",
        "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "        else:\n",
        "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
        "        return x\n",
        "\n",
        "    def call(self, queries, keys, values, mask=None):\n",
        "        # Rearrange the queries to be able to compute all heads in parallel\n",
        "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange the keys to be able to compute all heads in parallel\n",
        "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange the values to be able to compute all heads in parallel\n",
        "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
        "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange back the output into concatenated form\n",
        "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
        "\n",
        "        # Apply one final linear projection to the output to generate the multi-head attention\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
        "        return self.W_o(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Encodings"
      ],
      "metadata": {
        "id": "oIo-BpW2lpTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(max_length, d_model):\n",
        "    position = np.arange(max_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pos_enc = np.zeros((max_length, d_model))\n",
        "\n",
        "    # Calculate the positional encoding\n",
        "    pos_enc[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_enc[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    return tf.convert_to_tensor(pos_enc, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "dp3kvIbSlo5Y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNk7A-7rrwh7"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "O18oTCIMrwIs"
      },
      "outputs": [],
      "source": [
        "def basic_attention_model(in_size, num_heads, dim, fh):\n",
        "    inputs = tf.keras.Input(shape=(in_size,))\n",
        "    x = tf.keras.layers.Reshape((in_size, 1))(inputs)\n",
        "\n",
        "    attention_output = MultiHeadAttention(num_heads, 36, 36, dim)(x,x,x)\n",
        "    flattened = tf.keras.layers.Flatten()(attention_output)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(flattened)\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "\n",
        "\n",
        "def mlp_attention_model(in_size, att_dim, num_heads, fh):\n",
        "    inputs = tf.keras.layers.Input(shape=(in_size,))\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(inputs)\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Reshape((in_size, 1))(x)\n",
        "\n",
        "    x = MultiHeadAttention(num_heads, 36, 36, att_dim)(x,x,x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "def positional_attention_model(in_size, att_dim, dim, num_heads, fh):\n",
        "    inputs = tf.keras.layers.Input(shape=(in_size,))\n",
        "    inputs = tf.keras.layers.Reshape((in_size, 1))(inputs)\n",
        "    pos_enc = positional_encoding(in_size, dim)\n",
        "    x = inputs + pos_enc\n",
        "\n",
        "    x = MultiHeadAttention(num_heads, 36, 36, att_dim)(x,x,x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "\n",
        "def mlp_positional_attention_model(in_size, att_dim, dim, num_heads, fh):\n",
        "    inputs = tf.keras.layers.Input(shape=(in_size,))\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(inputs)\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    inputs = tf.keras.layers.Reshape((in_size, 1))(x)\n",
        "    pos_enc = positional_encoding(in_size, dim)\n",
        "    x = inputs + pos_enc\n",
        "\n",
        "    x = MultiHeadAttention(num_heads, 36, 36, att_dim)(x,x,x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lstm_attention_model(in_size, num_heads, dim, fh):\n",
        "    inputs = tf.keras.Input(shape=(in_size,))\n",
        "    x = tf.keras.layers.Reshape((in_size, 1))(inputs)\n",
        "    x = tf.keras.layers.LSTM(36, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Reshape((in_size*36, 1))(x)\n",
        "\n",
        "    attention_output = MultiHeadAttention(num_heads, 36, 36, dim)(x,x,x)\n",
        "    flattened = tf.keras.layers.Flatten()(attention_output)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(flattened)\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "\n",
        "def mlp_lstm_attention_model(in_size, num_heads, dim, fh):\n",
        "    inputs = tf.keras.Input(shape=(in_size,))\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(inputs)\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Dense(in_size, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Reshape((in_size, 1))(x)\n",
        "\n",
        "    x = tf.keras.layers.LSTM(36, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Reshape((in_size*36, 1))(x)\n",
        "\n",
        "    attention_output = MultiHeadAttention(num_heads, 36, 36, dim)(x,x,x)\n",
        "    flattened = tf.keras.layers.Flatten()(attention_output)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(flattened)\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def attention_model(in_size, num_heads, dim, fh):\n",
        "    inputs = tf.keras.Input(shape=(in_size,))\n",
        "    x = tf.keras.layers.Reshape((in_size, 1))(inputs)\n",
        "    print(shape(x))\n",
        "    attention_output = MultiHeadAttention(num_heads, 36, 36, dim)(x,x,x)\n",
        "    flattened = tf.keras.layers.Flatten()(attention_output)\n",
        "    x_b1 = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(flattened)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x_b1)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    x_b2 = tf.keras.layers.Add()([x_b1,x])\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x_b2)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    x_b3 = tf.keras.layers.Add()([x_b2,x])\n",
        "\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x_b3)\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def additive_lstm_attention_model(in_size, num_heads, dim, lstm_units, fh):\n",
        "    inputs = tf.keras.Input(shape=(in_size,))\n",
        "    x = tf.keras.layers.Reshape((in_size, 1))(inputs)\n",
        "    return x\n",
        "\n",
        "\n",
        "def positional_attention_model(in_size, att_dim, dim, num_heads, fh):\n",
        "    inputs = tf.keras.layers.Input(shape=(in_size,))\n",
        "    inputs = tf.keras.layers.Reshape((in_size, 1))(inputs)\n",
        "    pos_enc = positional_encoding(in_size, dim)\n",
        "    x = inputs + pos_enc\n",
        "    print(shape(x))\n",
        "    x = MultiHeadAttention(num_heads, 36, 36, att_dim)(x,x,x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x1 = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x1)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x2 = tf.keras.layers.Add()([x1,x])\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x2)\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x)\n",
        "    print(shape(lout))\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def transformer_model(in_size, att_dim, dim, num_heads, fh):\n",
        "    #Inputs\n",
        "    inputs = tf.keras.layers.Input(shape=(in_size,))\n",
        "    inputs = tf.keras.layers.Reshape((in_size, 1))(inputs)\n",
        "\n",
        "    #Encoder\n",
        "    pos_enc = positional_encoding(in_size, dim)\n",
        "    x_embedded = inputs + pos_enc\n",
        "    x = MultiHeadAttention(num_heads, 36, 36, att_dim)(x_embedded,x_embedded,x_embedded)\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x_add = tf.keras.layers.Add()([x_embedded,x])\n",
        "    x = tf.keras.layers.Flatten()(x_add)\n",
        "    x1 = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x1)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x2 = tf.keras.layers.Add()([x1,x])\n",
        "    lout = tf.keras.layers.Dense(fh, activation='linear', kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=lout)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdLfN_w2sDwf"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk_b_8I6KFwZ",
        "outputId": "e65b8129-a92d-44de-acbe-fb3299eeefe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc28NQhXsGVs",
        "outputId": "10ba72f3-ab5d-427c-97c0-eb6f6fed5931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating forecasts from network: 1\n",
            "Generating forecasts from network: 2\n",
            "Generating forecasts from network: 3\n",
            "Generating forecasts from network: 4\n",
            "Generating forecasts from network: 5\n",
            "Saved forecasts. File shape: (10220, 38)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "import os\n",
        "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
        "\n",
        "# import data_generators\n",
        "# import nn_models\n",
        "# from nn_models import *\n",
        "\n",
        "\n",
        "\n",
        "def model_train(train_path, m_path_prefix, model_id, in_size, frc_horizon, layers_num, layers_size, batch_size, steps):\n",
        "    # Training step implementation\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x, training=True)\n",
        "            # loss_f = tf.keras.losses.MeanSquaredError()\n",
        "            loss_f = tf.keras.losses.MeanAbsoluteError()\n",
        "            loss_value = loss_f(y, logits)\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
        "        opt.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        return loss_value\n",
        "\n",
        "    # Training data\n",
        "    train_file = np.load(train_path)\n",
        "    train_set = ds_generator_historical(train_file, batch_size, frc_horizon)\n",
        "    train_set = iter(train_set)\n",
        "\n",
        "    # Model\n",
        "    # if layers_num==3:\n",
        "    #     model = nn_models.shallow_mlp_model(in_size, layers_num, layers_size, frc_horizon)\n",
        "    # elif layers_num==1:\n",
        "    #     model = nn_models.shallow_mlp_model(in_size, layers_num, layers_size, frc_horizon)\n",
        "    # elif layers_num==10:\n",
        "    #     model = nn_models.deep_residual_mlp_model(in_size, layers_size, frc_horizon)\n",
        "    # else:\n",
        "    #     model = nn_models.deep_residual_mlp_model(in_size, layers_size, frc_horizon)\n",
        "\n",
        "    model = basic_attention_model(in_size, 6, 64, frc_horizon)                    #in_size, num_heads, att_dim, fh\n",
        "    #model = mlp_attention_model(in_size, 64, 6, frc_horizon)                      #in_size, att_dim, num_heads, fh\n",
        "    #model = positional_attention_model(in_size, 64, 1, 6, frc_horizon)            #in_size, att_dim, dim, num_heads, fh\n",
        "    #model = mlp_positional_attention_model(in_size, 64, 1, 6, frc_horizon)        #in_size, att_dim, dim, num_heads, fh\n",
        "    #model = lstm_attention_model(in_size, 6, 64, frc_horizon)                     #in_size, num_heads, att_dim, fh\n",
        "    #model = mlp_lstm_attention_model(in_size, 6, 64, frc_horizon)                 #in_size, num_heads, att_dim, fh\n",
        "\n",
        "\n",
        "    # print(model.summary())\n",
        "    # print(stop)\n",
        "\n",
        "    # Training parameters\n",
        "    number_of_steps = steps\n",
        "    init_lr = 0.001\n",
        "\n",
        "    opt = tf.optimizers.experimental.AdamW(learning_rate=init_lr)\n",
        "\n",
        "    # Training loop\n",
        "    print(f'\\nStarted training model: {(model_id + 1)}')\n",
        "    print(m_path_prefix)\n",
        "    for step in range(number_of_steps):\n",
        "        x_batch_train, y_batch_train = train_set.get_next()\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "        if step % 100 == 0:\n",
        "            print(f'Training loss at step {step}: {float(loss_value)}')\n",
        "\n",
        "    # Save model\n",
        "    m_path = m_path_prefix + str(model_id) + '.h5'\n",
        "    model.save(m_path)\n",
        "\n",
        "    # Clear memory\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    return 1\n",
        "\n",
        "def generate_forecasts(test_path, m_path_prefix, number_of_models, input_size, fh, save_path_prefix):\n",
        "    # Load test data\n",
        "    df_test = np.load(test_path)\n",
        "    x_test = df_test[:, -input_size:].astype(float)\n",
        "\n",
        "    y_hat_all = list([])\n",
        "    for i in range(number_of_models):\n",
        "        # Load model and forecast\n",
        "        print(f'Generating forecasts from network: {(i + 1)}')\n",
        "        # Load Model\n",
        "        pm = m_path_prefix + str(i) + '.h5'\n",
        "        model = tf.keras.models.load_model(pm,  custom_objects={'MultiHeadAttention': MultiHeadAttention(6, 36, 36, 64)}, compile=False) #custom_objects={'MultiHeadAttention': MultiHeadAttention(6, 36, 36, 128)}, '/content/drive/MyDrive/thesis/colab/models/attention_20000_steps_1.h5'\n",
        "        # Predict\n",
        "        y_hat_temp = model(x_test, training=False)\n",
        "        y_hat_all.append(y_hat_temp)\n",
        "\n",
        "\n",
        "    # Combine forecasts\n",
        "    y_hat_all = np.asarray(y_hat_all)\n",
        "    y_hat_all = np.median(y_hat_all, axis=0)\n",
        "\n",
        "    # Scale back forecasts\n",
        "    # x_min = df_test[:, 3:4].astype(float)\n",
        "    # x_max = df_test[:, 4:5].astype(float)\n",
        "    # y_hat_all = (y_hat_all * (x_max - x_min)) + x_min\n",
        "\n",
        "    x_mean = df_test[:, 3:4].astype(float)\n",
        "    x_std = df_test[:, 4:5].astype(float)\n",
        "    y_hat_all = (y_hat_all * x_std) + x_mean\n",
        "\n",
        "    # Save ensemble forecasts\n",
        "    df_predictions = pd.DataFrame({'Country': df_test[:, 1], 'Date': df_test[:, 2]})\n",
        "    df_predictions = pd.concat([df_predictions, pd.DataFrame(y_hat_all)], axis=1)\n",
        "\n",
        "    out_path = save_path_prefix + '.csv'\n",
        "    df_predictions.to_csv(out_path, index=None)\n",
        "    print(f'Saved forecasts. File shape: {df_predictions.shape}')\n",
        "\n",
        "\n",
        "# Experiment / Model parameters\n",
        "s_rate = 12 # sampling rate of the training data: every 24h, 12h, 1h\n",
        "f_h = 36 # forecasting horizon\n",
        "ins = 168 # number of past observations\n",
        "lnum = 7 # number of dense layers\n",
        "lsize = 512 # size of dense layers\n",
        "lossf = 'mae' # training loss\n",
        "batch = 512 # batch size #512 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "number_of_models = 5\n",
        "steps = 20000\n",
        "train_path = f'/content/drive/MyDrive/thesis/final/data/train_{s_rate}h_in{ins}_counorm.npy'\n",
        "test_path = f'/content/drive/MyDrive/thesis/final/data/x_test_in{ins}_counorm.npy'\n",
        "\n",
        "# # Train models\n",
        "# for nm in range(number_of_models):\n",
        "#     mpref = f'models/mlp_counorm_in{ins}_l{lnum}_{lsize}_{lossf}_{s_rate}h_'\n",
        "#     ret_val = model_train(train_path, mpref, nm, ins, f_h, lnum, lsize, batch)\n",
        "\n",
        "# Generate forecasts\n",
        "mpref = f'/content/drive/MyDrive/thesis/final/models/attention_{steps}_steps_'\n",
        "save_pref = f'/content/drive/MyDrive/thesis/final/forecasts/mlp_counorm_in{ins}_l{lnum}_{lsize}_{lossf}_{s_rate}h'\n",
        "generate_forecasts(test_path, mpref, number_of_models, ins, f_h, save_pref)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "q-uEtPr3rpwa"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}